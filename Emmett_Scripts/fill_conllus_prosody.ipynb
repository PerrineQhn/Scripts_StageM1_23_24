{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tgt\n",
    "import os\n",
    "import nltk\n",
    "import cmudict\n",
    "import sys\n",
    "\n",
    "current_path = os.getcwd()\n",
    "scripts_path = os.path.abspath(os.path.join(current_path, '..', '..', 'Python_Stage_23_24'))\n",
    "\n",
    "sys.path.append(scripts_path)\n",
    "\n",
    "import conll3\n",
    "import re\n",
    "\n",
    "def dico2FeatureString(dico):\n",
    "    \"\"\"Transforms a dictionary of Conll features into a string\"\"\"\n",
    "    feature = [\"=\".join([key, dico[key]]) for key in dico]\n",
    "    feature = \"|\".join(feature)\n",
    "    return feature\n",
    "\n",
    "def build_feature_dico(misc_features_string):\n",
    "    \"\"\"Turns a string of CONLL features into a callable dictionary\"\"\"\n",
    "    feature_dico = {}\n",
    "    for feature in misc_features_string.split(\"|\"):\n",
    "        key, value = feature.split(\"=\")\n",
    "        feature_dico[key] = value\n",
    "    return feature_dico\n",
    "\n",
    "def confirm_alignment(ref_interval, target_interval):\n",
    "    \"\"\"Takes two pitchtier intervals as entry and returns whether or not there is a temporal overlap, as well as the nature of that overlap.\"\"\"\n",
    "    \n",
    "    left_overlap = False\n",
    "    right_overlap = False\n",
    "    \n",
    "    # print(ref_interval, target_interval)\n",
    "    if ref_interval.text == \"\":\n",
    "        # print(\"empty string\") # debug\n",
    "        return False, left_overlap, right_overlap  # disregard pauses\n",
    "\n",
    "    if round(target_interval.end_time - ref_interval.start_time, 3) <= 0.001: \n",
    "        # print(\"A\") # debug\n",
    "        return False, left_overlap, right_overlap  \n",
    "    \n",
    "    elif round(target_interval.start_time - ref_interval.end_time, 3) >= -0.001: \n",
    "        # print(\"B\") # debug \n",
    "        return False, left_overlap, right_overlap  \n",
    "    \n",
    "    if  round(ref_interval.start_time - target_interval.start_time, 3) >= 0.01:\n",
    "        # print(\"C\") # debug\n",
    "        left_overlap = True\n",
    "\n",
    "    if round(target_interval.end_time - ref_interval.end_time, 3)  >= 0.01:\n",
    "        # print(\"D\") # debug\n",
    "        right_overlap = True\n",
    "\n",
    "    return True, left_overlap, right_overlap \n",
    "\n",
    "\n",
    "def vowel_final(word_string, dico):\n",
    "    \"\"\"Takes as entry an orthographic word string and a pronunciation dictionary, and confirms whether or not the canonical pronunciation of a that word ends in a vowel\"\"\"\n",
    "    vowels=\"aeiouAEIOU\" \n",
    "    if word_string == \"\":\n",
    "        return False\n",
    "    try:\n",
    "        phonemes = pronunciation[word_string.lower()][0]\n",
    "        if phonemes[-1][0] in vowels:\n",
    "            return True\n",
    "    except:\n",
    "        if word_string[-1] in vowels:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def extractProsodicAnnotation(textgrid, label_tier, label_tier2, token_tier, word_text_tier, syllable_transcription_tier):\n",
    "    \"\"\"\n",
    "    Extracts prosodic information from a .TextGrid file containing SLAM annotations and returns a dictionary of annotations.\n",
    "\n",
    "    Takes as entry:\n",
    "    * A textgrid file containing tiers corresponding to the following elements.\n",
    "    * Name of the tier containing phonetic transcriptions of syllables (syllable_transcription_tier)\n",
    "    * Names of two tiers containing global (label_tier) and local (label_tier2) SLAM annotations of syllables \n",
    "    * Name of the tier containing a token-level alignment and numeric ID in format 2:13 (utterance two, token 13) (token_tier)\n",
    "    * Name of tier containing orthographic transcription of token (word_text_tier)\n",
    "    \"\"\"\n",
    "\n",
    "    pronunciation = cmudict.dict()\n",
    "    dico = {}\n",
    "    \n",
    "    try: textgrid_object = tgt.read_textgrid(textgrid, include_empty_intervals=True, encoding=\"utf-8\")\n",
    "    except: textgrid_object = tgt.read_textgrid(textgrid, include_empty_intervals=True, encoding=\"utf-16\")\n",
    "    label_obj = textgrid_object.get_tier_by_name(label_tier)\n",
    "    label_obj2 = textgrid_object.get_tier_by_name(label_tier2)\n",
    "    token_object = textgrid_object.get_tier_by_name(token_tier)  \n",
    "    word_text_object = textgrid_object.get_tier_by_name(word_text_tier)\n",
    "    syllable_transcription_object = textgrid_object.get_tier_by_name(syllable_transcription_tier)\n",
    "    \n",
    "    i=0  # Index for label_obj\n",
    "    last_i = 0 # Index for label_obj to keep track of last alignment\n",
    "    for index, token in enumerate(token_object): \n",
    "        skip_features = False\n",
    "        \n",
    "        alignment_found = False\n",
    "        #alignment = False\n",
    "        syl_number = 1\n",
    "        \n",
    "        if not token.text: continue\n",
    "        \n",
    "        dico[token.text] = {}\n",
    "        dico[token.text][\"AlignBegin\"] = str(int(token.start_time*1000))\n",
    "        dico[token.text][\"AlignEnd\"] = str(int(token.end_time*1000))\n",
    "        \n",
    "        alignment, left_overlap, right_overlap = confirm_alignment(token, label_obj[i])\n",
    "\n",
    "        # print(f\"Token: {token}, Label: {label_obj[i]}, i: {i}, Alignment: {alignment}\")  # Debug\n",
    "        # print(f\"Token start: {token.start_time}, Token end: {token.end_time}\")\n",
    "        # print(f\"Label start: {label_obj[i].start_time}, Label end: {label_obj[i].end_time}\")\n",
    "        \n",
    "        # print(f\"i = {i}\")\n",
    "        while not alignment and i < len(label_obj) - 1:\n",
    "            i += 1\n",
    "            # print(f\"i = {i}\")  # Debug\n",
    "            # print(f\"Alignment not found for {token.text}, skipping\") # debug\n",
    "            # print(f\"Checking next interval: i: {i}, Alignment: {alignment}\")  # Debug\n",
    "            # print(f\"Next Label start: {label_obj[i].start_time}, Next Label end: {label_obj[i].end_time}\\n\")  # Debug\n",
    "            alignment, left_overlap, right_overlap = confirm_alignment(token, label_obj[i])\n",
    "\n",
    "        if not alignment:\n",
    "            # If no alignment is found, skip the token and move on to the next one\n",
    "            i = last_i\n",
    "            continue\n",
    "\n",
    "        last_i = i\n",
    "        while alignment:\n",
    "            skip_features = False\n",
    "            if left_overlap:\n",
    "                 ### Debug ###\n",
    "                #print(dico[token.text])\n",
    "                #print(dico[token_object[index-1].text])\n",
    "                #print(token.text, token_object[index-1].text)\n",
    "                #print(token, label_obj[i])\n",
    "                #print(token_object[index-1].text)\n",
    "                #print(left_overlap)\n",
    "\n",
    "                try: \n",
    "                    if vowel_final(word_text_object[index-1].text, pronunciation): \n",
    "                        dico[token.text][\"Syl\"+str(syl_number)] = \"FUSED\"\n",
    "                        skip_features = True\n",
    "\n",
    "                    ### Handles rare cases of triple fusions betweeen syllables\n",
    "\n",
    "                    elif dico[token_object[index-1].text][\"Syl1\"] == \"FUSED\" and \"Syl2\" not in dico[token_object[index-1].text]:\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)] = \"FUSED\"\n",
    "                        skip_features = True\n",
    "\n",
    "                    elif \"Syl1AlignBegin\" in dico[token_object[index-1].text] and str(dico[token_object[index-1].text]['Syl1AlignBegin']) == str(int(label_obj[i].start_time*1000)):\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)] = \"FUSED\"\n",
    "                        skip_features = True\n",
    "\n",
    "                    else:\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)+\"ExternalOnset\"] = \"True\"\n",
    "                except:\n",
    "                    print(\"problem on {}, confirm output\".format(label_obj[i]))\n",
    "\n",
    "                #dico[token.text][\"LeftOverlap\"] = \"True\"\n",
    "                \n",
    "            if right_overlap:\n",
    "                if vowel_final(word_text_object[index].text, pronunciation) != True and label_obj[i].start_time > token.start_time:  \n",
    "                    skip_features = True\n",
    "\n",
    "                #dico[token.text][\"RightOverlap\"] = \"True\"\n",
    "                \n",
    "            if skip_features == False:\n",
    "            \n",
    "                if label_obj[i].text:\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)+\"Glo\"] = label_obj[i].text\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)+\"Loc\"] = label_obj2[i].text\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)+\"Duration\"] = str(int((label_obj[i].end_time - label_obj[i].start_time)*1000))\n",
    "                    \n",
    "                    feature_dico = contourToFeatures(label_obj[i].text, \"Glo\")\n",
    "                    feature_dico.update(contourToFeatures(label_obj2[i].text, \"Loc\"))\n",
    "\n",
    "                    for feature in feature_dico.keys():\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)+feature] = feature_dico[feature]\n",
    "                    dico[token.text][\"SyllableCount\"] = str(syl_number)\n",
    "\n",
    "                    #dico[token.text][\"SylStart\"] = label_obj[i].start_time\n",
    "\n",
    "                else: \n",
    "                    if \"#\" not in word_text_object[index].text:\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)+\"Glo\"] = \"X\"\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)+\"Loc\"] = \"X\"\n",
    "                        #dico[token.text][\"SylStart\"] = label_obj[i].start_time\n",
    "                \n",
    "                if \"#\" not in word_text_object[index].text:\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)+\"AlignBegin\"] = str(int(label_obj[i].start_time*1000))\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)+\"AlignEnd\"] = str(int(label_obj[i].end_time*1000))\n",
    "\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)] = syllable_transcription_object[i].text\n",
    "\n",
    "\n",
    "            syl_number+=1\n",
    "            i+=1\n",
    "            if i == len(label_obj):\n",
    "                break\n",
    "                \n",
    "            alignment, left_overlap, right_overlap = confirm_alignment(token, label_obj[i])\n",
    "            #print(alignment, left_overlap, right_overlap)\n",
    "        \n",
    "        i-=1\n",
    "                \n",
    "    return dico\n",
    "\n",
    "def translate_code(code):\n",
    "    \"\"\"Takes in entry a textual SLAM label and converts it into a list of numeric values to facilitate extraction of features\"\"\"\n",
    "\n",
    "    translated_code = []\n",
    "    for letter in code[0:2]:\n",
    "        if letter == \"L\":\n",
    "            translated_code.append(1)\n",
    "        elif letter == \"l\":\n",
    "            translated_code.append(2)\n",
    "        elif letter == \"m\":\n",
    "            translated_code.append(3)\n",
    "        elif letter == \"h\":\n",
    "            translated_code.append(4)\n",
    "        elif letter == \"H\":\n",
    "            translated_code.append(5)\n",
    "\n",
    "    if len(code) > 2:\n",
    "        if code[2] == \"L\":\n",
    "            translated_code.append((1, int(code[3])))\n",
    "        elif code[2] == \"l\":\n",
    "            translated_code.append((2, int(code[3])))\n",
    "        elif code[2] == \"m\":\n",
    "            translated_code.append((3, int(code[3])))\n",
    "        elif code[2] == \"h\":\n",
    "            translated_code.append((4, int(code[3])))\n",
    "        elif code[2] == \"H\":\n",
    "            translated_code.append((5, int(code[3])))\n",
    "\n",
    "    return translated_code\n",
    "            \n",
    "def contourToFeatures(contour_label, suffix=\"\"):\n",
    "    \"\"\"Takes as entry a textual SLAM label and returns a set of categorical prosodic features describing the label.\"\"\"\n",
    "    \n",
    "    dico = {}\n",
    "\n",
    "    code = translate_code(contour_label)\n",
    "\n",
    "    if code[0] == code[1]:\n",
    "        dico[\"Slope\"+suffix] = \"Flat\"\n",
    "    elif code[0] < code[1]:\n",
    "        dico[\"Slope\"+suffix] = \"Rise\"\n",
    "    elif code[0] > code[1]:\n",
    "        dico[\"Slope\"+suffix] = \"Fall\"\n",
    "\n",
    "    if len(code) == 2:\n",
    "        height = (code[0] + code[1]) / 2\n",
    "    if len(code) == 3:\n",
    "        height = (code[0] + code[1] + code[2][0]) / 3\n",
    "\n",
    "    if height >= 3.5:\n",
    "        dico[\"AvgHeight\"+suffix] = \"H\"\n",
    "    elif height < 2.5:\n",
    "        dico[\"AvgHeight\"+suffix] = \"L\"\n",
    "    else: \n",
    "        dico[\"AvgHeight\"+suffix] = \"M\"\n",
    "\n",
    "    amplitude = abs(code[0] - code[1])\n",
    "    if amplitude <= 1:\n",
    "        dico[\"PitchRange\"+suffix] = \"L\"\n",
    "    elif amplitude >= 3:\n",
    "        dico[\"PitchRange\"+suffix] = \"H\"\n",
    "    else: \n",
    "        dico[\"PitchRange\"+suffix] = \"M\"\n",
    "        \n",
    "    return dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronunciation = cmudict.dict() # Pronunciation dictionary: to modify according to the language chosen\n",
    "syl_tier = \"SyllablesStyleGlo\" # Tier containing global contour\n",
    "syl_tier2 = \"SyllablesStyleLoc\" # Tier containing local contour\n",
    "word_tier = \"Word-ID\" # Tier containing numeric ID for token \n",
    "word_text_tier = \"Word-Text\" # Tier containing text token text \n",
    "syllable_text_tier = \"Syllables\" # Tier containing syllabic transcriptions \n",
    "slam_files = glob.glob(\"SLAM_output/*.TextGrid\") # Folder containing SLAM labels in TextGrid format\n",
    "conll_infiles = glob.glob(\"CONLL_files/gold_nongold/*.conllu\") # Folder containing CONLLU files to which prosodic information will be added\n",
    "conll_outfolder = \"CONLL_outfiles/gold_nongold/\"\n",
    "\n",
    "# If running this script a second time, useful for renaming features \n",
    "feature_rename_dict = {\"OldFeatname\" : \"NewFeatname\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill CONLLU files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treating ABJ_GWA_03\n",
      "treating ABJ_GWA_06\n",
      "treating ABJ_GWA_08\n",
      "treating ABJ_GWA_09\n",
      "treating ABJ_GWA_10\n",
      "treating ABJ_GWA_12\n",
      "treating ABJ_GWA_14\n",
      "treating ABJ_INF_02\n",
      "treating ABJ_INF_04\n",
      "treating ABJ_INF_05\n",
      "treating ABJ_INF_07\n",
      "treating ABJ_INF_09a\n",
      "treating ABJ_INF_09b\n",
      "treating ABJ_INF_10\n",
      "treating ABJ_INF_11\n",
      "treating ABJ_INF_12\n",
      "treating ABJ_NOU_02\n",
      "treating ABJ_NOU_04\n",
      "treating ABJ_NOU_05\n",
      "treating BEN_01\n",
      "treating BEN_02\n",
      "treating BEN_03\n",
      "treating BEN_04\n",
      "problem on Interval(174.199, 174.443, \"\"), confirm output\n",
      "treating BEN_08\n",
      "treating BEN_09\n",
      "treating BEN_12\n",
      "treating BEN_14\n",
      "treating BEN_34\n",
      "treating BEN_36\n",
      "treating ENU_01\n",
      "treating ENU_02\n",
      "treating ENU_04\n",
      "treating ENU_07\n",
      "treating ENU_08\n",
      "treating ENU_09\n",
      "treating ENU_11\n",
      "problem on Interval(100.34, 101.073, \"\"), confirm output\n",
      "treating ENU_14\n",
      "treating ENU_15\n",
      "treating ENU_22\n",
      "treating ENU_24\n",
      "problem on Interval(93.942, 95.671, \"\"), confirm output\n",
      "treating ENU_26\n",
      "problem on Interval(72.408, 73.07, \"\"), confirm output\n",
      "treating ENU_28\n",
      "treating ENU_33\n",
      "treating ENU_34\n",
      "treating ENU_35\n",
      "treating ENU_36\n",
      "treating ENU_37\n",
      "treating IBA_01\n",
      "treating IBA_02\n",
      "treating IBA_03\n",
      "treating IBA_04\n",
      "treating IBA_12\n",
      "problem on Interval(212.634, 213.139, \"\"), confirm output\n",
      "treating IBA_13\n",
      "treating IBA_15\n",
      "treating IBA_18\n",
      "treating IBA_19\n",
      "treating IBA_20\n",
      "treating IBA_21\n",
      "treating IBA_23\n",
      "treating IBA_31\n",
      "treating IBA_32\n",
      "treating IBA_33\n",
      "treating IBA_34\n",
      "treating IBA_38\n",
      "problem on Interval(212.527, 212.957, \"\"), confirm output\n",
      "treating IBA_40\n",
      "treating IBA_41\n",
      "treating JOS_01\n",
      "treating JOS_05\n",
      "treating JOS_08\n",
      "treating JOS_09\n",
      "treating JOS_12\n",
      "treating JOS_13\n",
      "treating JOS_14\n",
      "treating JOS_15\n",
      "treating JOS_16\n",
      "treating JOS_17\n",
      "treating JOS_18\n",
      "treating JOS_19\n",
      "treating JOS_20\n",
      "treating JOS_24\n",
      "treating JOS_25\n",
      "treating JOS_35\n",
      "treating JOS_36\n",
      "treating JOS_37\n",
      "treating JOS_38\n",
      "treating JOS_39\n",
      "problem on Interval(29.369, 29.929, \"\"), confirm output\n",
      "treating KAD_02\n",
      "treating KAD_03\n",
      "treating KAD_09\n",
      "treating KAD_10\n",
      "treating KAD_11\n",
      "treating KAD_12\n",
      "treating KAD_13\n",
      "treating KAD_15\n",
      "treating KAD_17\n",
      "treating KAD_20\n",
      "treating KAD_24\n",
      "problem on Interval(102.764, 102.99, \"\"), confirm output\n",
      "treating KAD_25\n",
      "treating KAD_26\n",
      "treating KAD_27\n",
      "treating KAD_28\n",
      "treating KAD_30\n",
      "treating KAD_31\n",
      "treating LAG_01\n",
      "treating LAG_03\n",
      "treating LAG_05\n",
      "problem on Interval(105.379, 107.549, \"\"), confirm output\n",
      "problem on Interval(200.903, 205.82, \"\"), confirm output\n",
      "treating LAG_07\n",
      "treating LAG_11\n",
      "treating LAG_12\n",
      "treating LAG_13\n",
      "treating LAG_15\n",
      "treating LAG_18\n",
      "treating LAG_21\n",
      "treating LAG_23\n",
      "treating LAG_24\n",
      "treating LAG_25\n",
      "problem on Interval(24.13, 24.32, \"ll\"), confirm output\n",
      "treating LAG_27\n",
      "treating LAG_31\n",
      "treating LAG_33\n",
      "treating LAG_34\n",
      "treating LAG_37\n",
      "treating ONI_01\n",
      "treating ONI_02\n",
      "treating ONI_03\n",
      "treating ONI_04\n",
      "treating ONI_05\n",
      "treating ONI_07\n",
      "treating ONI_09\n",
      "treating ONI_10\n",
      "treating ONI_17\n",
      "treating ONI_18a\n",
      "treating ONI_18b\n",
      "problem on Interval(294.87, 295.091, \"ll\"), confirm output\n",
      "treating ONI_24\n",
      "treating ONI_26\n",
      "treating ONI_27\n",
      "treating PRT_01\n",
      "problem on Interval(314.794, 315.54, \"\"), confirm output\n",
      "problem on Interval(374.904, 375.394, \"\"), confirm output\n",
      "problem on Interval(374.904, 375.394, \"\"), confirm output\n",
      "treating PRT_02\n",
      "treating PRT_04\n",
      "treating PRT_05\n",
      "treating PRT_07\n",
      "treating PRT_08\n",
      "treating PRT_09\n",
      "treating PRT_11\n",
      "treating WAZA_01\n",
      "treating WAZA_03\n",
      "treating WAZA_05\n",
      "treating WAZA_08\n",
      "treating WAZA_09\n",
      "treating WAZA_10\n",
      "treating WAZA_12\n",
      "treating WAZK_02\n",
      "problem on Interval(193.057, 193.287, \"mL\"), confirm output\n",
      "problem on Interval(220.903, 221.183, \"lm\"), confirm output\n",
      "treating WAZK_03\n",
      "treating WAZK_04\n",
      "treating WAZK_06\n",
      "treating WAZK_07\n",
      "treating WAZK_08\n",
      "treating WAZK_09\n",
      "treating WAZK_10\n",
      "treating WAZK_11\n",
      "treating WAZK_12\n",
      "treating WAZL_02\n",
      "treating WAZL_03\n",
      "treating WAZL_05\n",
      "treating WAZL_08\n",
      "treating WAZL_12\n",
      "treating WAZL_13\n",
      "treating WAZL_15\n",
      "treating WAZP_01\n",
      "treating WAZP_02\n",
      "treating WAZP_03\n",
      "treating WAZP_04\n",
      "treating WAZP_05\n",
      "treating WAZP_06\n",
      "treating WAZP_07\n",
      "treating WAZP_08\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for slam_file in sorted(slam_files):\n",
    "    basename = os.path.basename(slam_file)[:-9]\n",
    "    print(\"treating\", basename)\n",
    "    # if \"IBA_03\" in basename:\n",
    "    annotations = extractProsodicAnnotation(slam_file, syl_tier, syl_tier2, word_tier, word_text_tier, syllable_text_tier)\n",
    "    for infile in conll_infiles:\n",
    "        \n",
    "        if os.path.basename(infile)[:len(basename)] == basename:\n",
    "            trees=conll3.conllFile2trees(infile)\n",
    "            for treei, tree in enumerate(trees):\n",
    "                for token in tree:\n",
    "                    identifier = str(treei+1)+\":\"+str(token)\n",
    "                    misc_features = tree[token]['misc']\n",
    "                    feature_dico = build_feature_dico(misc_features)\n",
    "                    \n",
    "                    \n",
    "                    features_to_delete = []\n",
    "                    \n",
    "                    new_feature_dico = {}\n",
    "                    for feature in feature_dico.keys():\n",
    "                        #print(feature)\n",
    "                        if re.match(\"Syl[0-9].*\", feature):\n",
    "                            if \"Amplitude\" not in feature:\n",
    "                                features_to_delete.append(feature)\n",
    "                            #print(features_to_delete)\n",
    "                            \n",
    "                        #print(feature)\n",
    "                        if feature[4:] in feature_rename_dict.keys():\n",
    "                            newfeat = feature.replace(feature[4:], feature_rename_dict[feature[4:]])\n",
    "                            #print(feature, newfeat)\n",
    "                            newval = feature_dico[feature]\n",
    "                            new_feature_dico[newfeat] = newval\n",
    "                            features_to_delete.append(feature)\n",
    "                            #print(feature_dico)\n",
    "\n",
    "                        \"\"\"\"   \n",
    "                        if feature == \"F0Enonce\":\n",
    "                            newfeat = feature.replace(feature, feature_rename_dict[feature])\n",
    "                            #print(feature, newfeat)\n",
    "                            newval = feature_dico[feature]\n",
    "                            new_feature_dico[newfeat] = newval\n",
    "                            features_to_delete.append(feature)\n",
    "                            #print(feature_dico)\n",
    "                        \"\"\"\n",
    "                            \n",
    "                    for feature in new_feature_dico.keys():\n",
    "                        feature_dico[feature] = new_feature_dico[feature]\n",
    "                        \n",
    "                    \n",
    "                    for feature in features_to_delete:\n",
    "                        if feature in feature_dico.keys():\n",
    "                            del feature_dico[feature]\n",
    "                    #print(feature_dico)\n",
    "                    \n",
    "                        \n",
    "                    #print(feature_dico)\n",
    "                    if identifier in annotations.keys():\n",
    "                        for item in annotations[identifier]:\n",
    "                            #print(identifier, item)\n",
    "                            feature_dico[item] = annotations[identifier][item]\n",
    "                        \n",
    "                        if \"Syl1\" in feature_dico.keys() and feature_dico[\"Syl1\"] == \"FUSED\":\n",
    "                            #print(\"hi\")\n",
    "                            if \"Syl1Duration\" in feature_dico.keys():\n",
    "                                del feature_dico[\"Syl1Duration\"]\n",
    "                            if \"Syl1MeanF0\" in feature_dico.keys():\n",
    "                                del feature_dico[\"Syl1MeanF0\"]\n",
    "                            if \"Syl1SemitonesFromUtteranceMean\" in feature_dico.keys():\n",
    "                                del feature_dico[\"Syl1SemitonesFromUtteranceMean\"]\n",
    "                        #print(identifier)\n",
    "                        #print(feature_string)\n",
    "                        feature_string = dico2FeatureString(feature_dico)\n",
    "                        tree[token]['misc'] = feature_string\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    identifier = str(treei+1)+\":\"+str(token)\n",
    "                    misc_features = tree[token]['misc']\n",
    "                    \n",
    "                    if identifier in annotations.keys():              \n",
    "                        for item in annotations[identifier]:\n",
    "                            misc_features = misc_features + \"|\"\n",
    "                            featstring = item+\"=\"+annotations[identifier][item]\n",
    "                            misc_features = misc_features + featstring\n",
    "                    tree[token]['misc'] = misc_features\n",
    "                    #print(tree[token]['misc'])\n",
    "                    \"\"\"\n",
    "                    \n",
    "            conll3.trees2conllFile(trees, conll_outfolder+os.path.basename(infile))\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
