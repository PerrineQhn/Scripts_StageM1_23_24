{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Description des fonctions et des paramètres :\n",
    "\n",
    "1. **Imports et Configuration de base**\n",
    "   - Importation des bibliothèques nécessaires : `re`, `os`, `sys`, `textgrid` de `praatio`, et `outils.conll3`.\n",
    "   - Ajout du chemin dynamique vers le dossier 'scripts'.\n",
    "\n",
    "2. **Fonctions**\n",
    "   - **load_textgrid(textgrid: str, alignment=False) -> dict**\n",
    "     - Récupère les phrases en tant que dictionnaires à partir d'un fichier TextGrid.\n",
    "     - Paramètres :\n",
    "       - `textgrid`: Le chemin vers le fichier TextGrid.\n",
    "       - `alignment`: Booléen pour indiquer si les alignements des mots doivent être inclus.\n",
    "     - Retourne :\n",
    "       - Un dictionnaire contenant les phrases.\n",
    "\n",
    "   - **update_sent_text_conllu(conllu: str, sentences: dict, out: str)**\n",
    "     - Met à jour les phrases dans un fichier CONLL-U en utilisant les phrases fournies dans un dictionnaire.\n",
    "     - Paramètres :\n",
    "       - `conllu`: Le chemin vers le fichier CONLL-U.\n",
    "       - `sentences`: Le dictionnaire contenant les phrases à mettre à jour.\n",
    "       - `out`: Le chemin vers le fichier de sortie pour enregistrer le fichier CONLL-U mis à jour.\n",
    "     - Retourne :\n",
    "       - Deux dictionnaires contenant les anciennes et nouvelles phrases.\n",
    "\n",
    "   - **compare_and_replace(old_tokens, new_tokens)**\n",
    "     - Remplace les tokens modifiés dans les nouvelles annotations par les anciennes annotations et ajoute les tokens manquants des nouvelles annotations.\n",
    "     - Paramètres :\n",
    "       - `old_tokens`: Les anciennes annotations.\n",
    "       - `new_tokens`: Les nouvelles annotations.\n",
    "     - Retourne :\n",
    "       - Les nouvelles annotations avec les tokens modifiés remplacés par les anciennes annotations.\n",
    "\n",
    "   - **update_conllu_id(conllu_path: str, output_path: str, old_sent: dict) -> None**\n",
    "     - Met à jour les index des tokens et ajuste les index HEAD dans un fichier CONLL-U.\n",
    "     - Paramètres :\n",
    "       - `conllu_path`: Le chemin du fichier CONLL-U à mettre à jour.\n",
    "       - `output_path`: Le chemin du fichier de sortie pour enregistrer le fichier CONLL-U mis à jour.\n",
    "       - `old_sent`: Le dictionnaire contenant les phrases précédentes.\n",
    "\n",
    "   - **correct_alignements(sentences: dict, conllu_path: str)**\n",
    "     - Corrige les alignements dans un fichier CONLL-U en utilisant les alignements fournis dans un dictionnaire.\n",
    "     - Paramètres :\n",
    "       - `sentences`: Le dictionnaire contenant les alignements.\n",
    "       - `conllu_path`: Le chemin du fichier CONLL-U à corriger.\n",
    "\n",
    "   - **adjust_alignments_delete_x(conllu_path: str)**\n",
    "     - Ajuste les alignements dans un fichier CONLL-U en supprimant les valeurs 'X' et en les remplaçant par des valeurs valides.\n",
    "     - Paramètres :\n",
    "       - `conllu_path`: Le chemin du fichier CONLL-U à ajuster.\n",
    "\n",
    "##### Paramètres globaux :\n",
    "\n",
    "- `input_textgrid_directory`: Répertoire contenant les fichiers TextGrid.\n",
    "- `input_conllu_directory`: Répertoire contenant les fichiers CONLL-U.\n",
    "- `conllu_output_directory`: Répertoire où les fichiers CONLL-U mis à jour seront enregistrés.\n",
    "\n",
    "##### Processus principal :\n",
    "\n",
    "1. **Chargement et mise à jour des fichiers CONLL-U**\n",
    "   - Le script parcourt chaque sous-dossier dans `input_conllu_directory`.\n",
    "   - Pour chaque fichier CONLL-U, il charge le fichier TextGrid correspondant.\n",
    "   - Les phrases du fichier CONLL-U sont mises à jour avec les nouvelles phrases extraites du fichier TextGrid.\n",
    "   - Les index des tokens sont mis à jour et les alignements sont corrigés.\n",
    "\n",
    "2. **Correction des alignements**\n",
    "   - Les alignements des phrases sont corrigés en utilisant les informations d'alignement fournies dans les fichiers TextGrid.\n",
    "   - Les valeurs 'X' dans les alignements sont supprimées et remplacées par des valeurs valides.\n",
    "\n",
    "##### Résumé :\n",
    "Le script met à jour les phrases dans les fichiers CONLL-U en utilisant les nouvelles phrases extraites des fichiers TextGrid, corrige les index des tokens et les alignements, et enregistre les fichiers CONLL-U mis à jour dans un répertoire de sortie. Les alignements des phrases sont corrigés en utilisant les informations d'alignement fournies dans les fichiers TextGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_path = os.getcwd()\n",
    "scripts_path = os.path.join(current_path, 'scripts')\n",
    "\n",
    "# Ajoutez le chemin vers le dossier 'scripts' dynamiquement\n",
    "sys.path.append(scripts_path)\n",
    "\n",
    "from outils.conll3 import *\n",
    "from praatio import textgrid as tgio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_textgrid(textgrid:str, alignment=False) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves sentences as dictionaries (file_name{sent_id{sent_text}, sent_id{sent_text}}}, with the Sent-ID tier having the ID of the sentence and Sent-Text tier having the text of the sentence)\n",
    "    \n",
    "    Parameters:\n",
    "    textgrid (str): Path to the textgrid file\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing the sentences\n",
    "    \"\"\"\n",
    "    tg = tgio.openTextgrid(textgrid, includeEmptyIntervals=False)\n",
    "    file_name = os.path.basename(textgrid).replace(\".TextGrid\", \"\")\n",
    "    sentences = {file_name: {}}\n",
    "\n",
    "    # Verify if the \"Sent-ID\" and \"Sent-Text\" tiers exist\n",
    "    if \"Sent-ID\" in tg.tierNames and \"Sent-Text\" in tg.tierNames and \"Word-ID\" in tg.tierNames and \"Word-Text\" in tg.tierNames:\n",
    "        sent_id_tier = tg.getTier(\"Sent-ID\")\n",
    "        sent_text_tier = tg.getTier(\"Sent-Text\")\n",
    "        word_id_tier = tg.getTier(\"Word-ID\")\n",
    "        word_text_tier = tg.getTier(\"Word-Text\")\n",
    "\n",
    "        if alignment == True:\n",
    "            for i in range(min(len(sent_id_tier.entries), len(sent_text_tier.entries))):\n",
    "                sent_id = sent_id_tier.entries[i].label\n",
    "                sent_text = sent_text_tier.entries[i].label\n",
    "                sentences[file_name][sent_id] = {\"text\": sent_text, \"words\": {}}\n",
    "                for j in range(min(len(word_id_tier.entries), len(word_text_tier.entries))):\n",
    "                    word_id = word_id_tier.entries[j].label\n",
    "                    word_text = word_text_tier.entries[j].label\n",
    "                    start = word_text_tier.entries[j].start\n",
    "                    end = word_text_tier.entries[j].end\n",
    "                    sentences[file_name][sent_id][\"words\"][word_id] = (start, end, word_text)\n",
    "        else:\n",
    "            for i in range(min(len(sent_id_tier.entries), len(sent_text_tier.entries))):\n",
    "                sent_id = sent_id_tier.entries[i].label\n",
    "                sent_text = sent_text_tier.entries[i].label\n",
    "                sentences[file_name][sent_id] = sent_text\n",
    "\n",
    "\n",
    "    # print(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sent_text_conllu(conllu: str, sentences: dict, out: str):\n",
    "    \"\"\"\n",
    "    Update the sentences in a CONLL-U file using the provided sentences in a dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    conllu (str): The path to the CONLL-U file.\n",
    "    sentences (dict): The dictionary containing the sentences to update.\n",
    "    out (str): The path to the output file to save the updated CONLL-U file.\n",
    "    \"\"\"\n",
    "    with open(conllu, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    updated_data = []\n",
    "    old_sent = {}\n",
    "    new_sent = {}\n",
    "    current_file = os.path.basename(conllu).replace(\".conllu\", \"\")\n",
    "    if current_file.startswith(\"ABJ\"):\n",
    "        textgrid_name = \"_\".join(current_file.split(\"_\")[:3])\n",
    "    else:\n",
    "        textgrid_name = \"_\".join(current_file.split(\"_\")[:2])\n",
    "\n",
    "    textgrid_name = textgrid_name + \"-merged\"\n",
    "    \n",
    "    for sentence in data.split(\"\\n\\n\"):\n",
    "        # print(data)\n",
    "        if sentence.strip():\n",
    "            lines = sentence.split(\"\\n\")\n",
    "            sent_id_match = re.search(r\"# sent_id = (.+)\", sentence)\n",
    "            if sent_id_match:\n",
    "                sent_id = sent_id_match.group(1)\n",
    "                sent_id = sent_id.split(\"__\")[1]\n",
    "                # print(textgrid_name, sentences[textgrid_name])\n",
    "                if textgrid_name in sentences and sent_id in sentences[textgrid_name]:\n",
    "                    new_text = sentences[textgrid_name][sent_id]\n",
    "                    for i, line in enumerate(lines):\n",
    "                        if line.startswith(\"# text =\"):\n",
    "                            old_text = line.split(\" = \")[1]\n",
    "                            lines[i] = f\"# text = {new_text}\"\n",
    "                            old_sent[sent_id] = old_text\n",
    "                            new_sent[sent_id] = new_text\n",
    "                            break\n",
    "                    \n",
    "            updated_data.append(\"\\n\".join(lines))\n",
    "\n",
    "    with open(out, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\\n\".join(updated_data) + \"\\n\")\n",
    "    \n",
    "    return old_sent, new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_replace(old_tokens, new_tokens):\n",
    "    \"\"\"\n",
    "    Replaces the modified tokens in the new annotations with the old annotations and adds the missing tokens from the new annotations.\n",
    "\n",
    "    Parameters:\n",
    "    old_tokens (dict): The old annotations.\n",
    "    new_tokens (dict): The new annotations.\n",
    "\n",
    "    Returns:\n",
    "    dict: The new annotations with the modified tokens replaced by the old annotations.\n",
    "    \"\"\"\n",
    "    new_tokens_dic = {}\n",
    "    new_key = 1\n",
    "\n",
    "    # print(\"\\nold \", old_tokens)\n",
    "    # print(\"new \", new_tokens)\n",
    "\n",
    "    # Dict to keep track of token occurrences\n",
    "    old_token_occurrences = {}\n",
    "    for idx, token in old_tokens.items():\n",
    "        token_upper = token.upper()\n",
    "        if token_upper not in old_token_occurrences:\n",
    "            old_token_occurrences[token_upper] = []\n",
    "        old_token_occurrences[token_upper].append(idx)\n",
    "\n",
    "    used_old_indices = set()\n",
    "\n",
    "    def get_context(tokens, idx):\n",
    "        \"\"\" Helper function to get the context of a token \"\"\"\n",
    "        prev_idx = idx - 1\n",
    "        while prev_idx in tokens and tokens[prev_idx] == '#':\n",
    "            prev_idx -= 1\n",
    "        prev_token = tokens.get(prev_idx, '')\n",
    "\n",
    "        next_idx = idx + 1\n",
    "        while next_idx in tokens and tokens[next_idx] == '#':\n",
    "            next_idx += 1\n",
    "        next_token = tokens.get(next_idx, '')\n",
    "\n",
    "        return prev_token.upper(), next_token.upper()\n",
    "\n",
    "    for new_idx, new_token in new_tokens.items():\n",
    "        new_token_upper = new_token.upper()\n",
    "        token_found = False\n",
    "\n",
    "        if new_token_upper in old_token_occurrences:\n",
    "            new_prev, new_next = get_context(new_tokens, new_idx)\n",
    "\n",
    "            for old_idx in old_token_occurrences[new_token_upper]:\n",
    "                if old_idx not in used_old_indices:\n",
    "                    old_prev, old_next = get_context(old_tokens, old_idx)\n",
    "\n",
    "                    if old_prev == new_prev and old_next == new_next:\n",
    "                        new_tokens_dic[new_key] = old_tokens[old_idx]\n",
    "                        # print(\"old=new \", new_tokens_dic)\n",
    "                        new_key += 1\n",
    "                        used_old_indices.add(old_idx)\n",
    "                        token_found = True\n",
    "                        break\n",
    "\n",
    "        if not token_found:\n",
    "            # Handling tokens with `~`\n",
    "            new_token_tild = new_token.replace(\"~\", \"\")\n",
    "            for old_idx, old_token in old_tokens.items():\n",
    "                old_token_tild = old_token.replace(\"~\", \"\")\n",
    "                if new_token_tild.upper() == old_token_tild.upper() and old_idx not in used_old_indices:\n",
    "                    old_prev, old_next = get_context(old_tokens, old_idx)\n",
    "                    new_prev, new_next = get_context(new_tokens, new_idx)\n",
    "\n",
    "                    if old_prev == new_prev and old_next == new_next:\n",
    "                        new_tokens_dic[new_key] = old_token\n",
    "                        # print(\"tild \", new_tokens_dic)\n",
    "                        new_key += 1\n",
    "                        used_old_indices.add(old_idx)\n",
    "                        token_found = True\n",
    "                        break\n",
    "\n",
    "        if not token_found:\n",
    "            new_tokens_dic[new_key] = new_token\n",
    "            # print(\"# or new_token \", new_tokens_dic)\n",
    "            new_key += 1\n",
    "\n",
    "    return new_tokens_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_conllu_id(conllu_path: str, output_path: str, old_sent: dict) -> None:\n",
    "    \"\"\"\n",
    "    Updates the token indexing and adjusts the HEAD indices in a CONLL-U file.\n",
    "    \n",
    "    Parameters:\n",
    "    conllu_path (str): The path of the CONLL-U file to update.\n",
    "    output_path (str): The output file path to save the updated CONLL-U file.\n",
    "    old_sent (dict): The dictionary containing the previous sentences.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open(conllu_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read().strip().split('\\n\\n')\n",
    "\n",
    "    processed_sentences = []\n",
    "\n",
    "    # Get the ID of the old sentence {sent_id: {word_id: word}}\n",
    "    old_dico = {}\n",
    "    for sent_id, sent_text in old_sent.items():\n",
    "        sent_text = sent_text.split()\n",
    "        dict_id_word = {i + 1: word for i, word in enumerate(sent_text)}\n",
    "        old_dico[sent_id] = dict_id_word\n",
    "\n",
    "    new_dico = {}\n",
    "    for sentence in data:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        metadata = []\n",
    "        token_lines = []\n",
    "        for line in lines:\n",
    "            if line.startswith('#'):\n",
    "                metadata.append(line)\n",
    "            else:\n",
    "                token_lines.append(line.split('\\t'))\n",
    "\n",
    "        sent_id = None\n",
    "        for m in metadata:\n",
    "            if m.startswith(\"# sent_id =\"):\n",
    "                sent_id = m.split(\" = \")[1]\n",
    "                sent_id = sent_id.split(\"__\")[1]\n",
    "                new_dico[sent_id] = {}\n",
    "                \n",
    "            if m.startswith(\"# text =\"):\n",
    "                sent_text = m.split(\" = \")[1]\n",
    "                sent_text = sent_text.split()\n",
    "                dict_id_word = {i + 1: word for i, word in enumerate(sent_text)}\n",
    "                new_dico[sent_id] = dict_id_word\n",
    "\n",
    "        # Check if sent_id is in old_dico and new_dico\n",
    "        if sent_id in old_dico and sent_id in new_dico:\n",
    "            # Correcting the token indices using the dictionary + adding missing tokens to token_lines\n",
    "            corrected_token_lines = []\n",
    "            old_tokens = old_dico[sent_id]\n",
    "            new_tokens = new_dico[sent_id]\n",
    "\n",
    "            new_tokens = compare_and_replace(old_tokens, new_tokens)\n",
    "\n",
    "            # print(\"\\n\\nAncienne phrase et ses mots avec ID:\", old_tokens)\n",
    "            # print(\"Nouvelle phrase et ses mots avec ID:\", new_tokens)\n",
    "            \n",
    "            # Dictionary to keep track of already used indices for each word\n",
    "            used_old_indices = {}\n",
    "\n",
    "            for new_idx, new_word in new_tokens.items():\n",
    "                # If the token itself is missing, we add it with the corresponding index or if the token is \"#\" and the index is different from the old token\n",
    "                # print(\"\\n\\n\",conllu_path)\n",
    "                # print(\"\\nNew index:\", new_idx, \"New word:\", new_word)\n",
    "                old_word_x = [old_word for old_idx, old_word in old_tokens.items() if old_idx == new_idx]\n",
    "                old_idx_x = [old_idx for old_idx, old_word in old_tokens.items() if old_idx == new_idx]\n",
    "                # print(\"Old word:\", old_word_x, \"Old index:\", old_idx_x)\n",
    "\n",
    "                new_word_not_in_old_tokens = new_word.upper() not in [old_word.upper() for old_word in old_tokens.values()]\n",
    "                new_word_is_hash_with_different_index = new_word == \"#\" and (not old_idx_x or new_idx != old_idx_x[0])\n",
    "                new_word_is_hash_different_old_word = new_word == \"#\" and old_word_x and old_word_x[0] != \"#\"\n",
    "\n",
    "                # Adding debug prints\n",
    "                # print(\"new_word_not_in_old_tokens:\", new_word_not_in_old_tokens)\n",
    "                # print(\"new_word_is_hash_with_different_index:\", new_word_is_hash_with_different_index)\n",
    "                # print(\"new_word_is_hash_different_old_word:\", new_word_is_hash_different_old_word)\n",
    "                # print(\"Condition principale:\", new_word_not_in_old_tokens or new_word_is_hash_with_different_index)\n",
    "\n",
    "\n",
    "\n",
    "                if new_word_not_in_old_tokens or new_word_is_hash_with_different_index or new_word_is_hash_different_old_word:\n",
    "                    # print(\"Token manquant:\", new_word)\n",
    "                    if new_word == \"#\":\n",
    "                        # print(conllu_path)\n",
    "                        # print(\"Sent ID:\", sent_id)\n",
    "                        # print(\"Ancienne phrase et ses mots avec ID:\", old_tokens)\n",
    "                        # print(\"Nouvelle phrase et ses mots avec ID:\", new_tokens)\n",
    "                        # print(\"Token manquant\", new_word)\n",
    "                        corrected_token_lines.append([str(new_idx), new_word, new_word, 'PUNCT', '_', '_', '_', '_', '_', '_'])\n",
    "                    else:\n",
    "                        # print(\"\\n\\n\",conllu_path)\n",
    "                        # print(\"Sent ID:\", sent_id)\n",
    "                        # print(\"Ancienne phrase et ses mots avec ID:\", old_tokens)\n",
    "                        # print(\"Nouvelle phrase et ses mots avec ID:\", new_tokens)\n",
    "                        # print(\"Token manquant\", new_word)\n",
    "                        corrected_token_lines.append([str(new_idx), new_word, new_word, '_', '_', '_', '_', '_', '_', '_'])\n",
    "                else:\n",
    "                    # If the token is present, update it with the corresponding index in the new dictionary\n",
    "                    matching_old_indices = [old_idx for old_idx, old_word in old_tokens.items() if old_word.upper() == new_word.upper()]\n",
    "\n",
    "                    try:\n",
    "                        # Find the first unused index\n",
    "                        available_old_idx = next(old_idx for old_idx in matching_old_indices if old_idx not in used_old_indices.get(new_word.upper(), []))\n",
    "                    except StopIteration:\n",
    "                        print(f\"Erreur: Pas d'indice disponible pour le mot '{new_word}' avec l'indice '{new_idx}'\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Update the dictionary of used indices\n",
    "                    if new_word.upper() not in used_old_indices:\n",
    "                        used_old_indices[new_word.upper()] = []\n",
    "                    used_old_indices[new_word.upper()].append(available_old_idx)\n",
    "                    \n",
    "                    if available_old_idx - 1 < len(token_lines):\n",
    "                        old_word = old_tokens[available_old_idx]\n",
    "                        corrected_token_lines.append([str(new_idx), old_word] + token_lines[available_old_idx - 1][2:])\n",
    "                    else:\n",
    "                        print(f\"Erreur: Indice {available_old_idx - 1} est hors limite pour les tokens dans la phrase avec sent_id = {sent_id}\")\n",
    "\n",
    "\n",
    "                # print(\"Corrected token lines:\", corrected_token_lines)\n",
    "\n",
    "            for token_line in corrected_token_lines:\n",
    "                head_idx = token_line[6]\n",
    "                if head_idx != \"0\" and head_idx != \"_\":\n",
    "                    head_idx = int(head_idx)\n",
    "                    for idx, word in new_tokens.items():\n",
    "                        # print(\"Index:\", idx, \"Word:\", word)\n",
    "                        if idx <= head_idx and word == '#' and idx not in used_old_indices.get('#', []):\n",
    "                            # print(\"Head index:\", head_idx, \"New head index:\", head_idx + 1)\n",
    "                            head_idx += 1\n",
    "                    new_head_idx = head_idx\n",
    "                    # print(\"Head index:\", head_idx, \"New head index:\", new_head_idx)\n",
    "                    token_line[6] = str(new_head_idx)\n",
    "\n",
    "\n",
    "            # Reformatting token lines\n",
    "            formatted_token_lines = ['\\t'.join(token_line) for token_line in corrected_token_lines]\n",
    "            processed_sentences.append('\\n'.join(metadata + formatted_token_lines))\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n\\n'.join(processed_sentences) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_alignements(sentences: dict, conllu_path: str):\n",
    "    \"\"\"\n",
    "    Correct the alignments in a CONLL-U file using the alignments provided in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (dict): The dictionary containing the alignments.\n",
    "    conllu_path (str): The path of the CONLL-U file to be corrected.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    punctuation_list = [\n",
    "        \">\",\n",
    "        \"<\",\n",
    "        \"//\",\n",
    "        \"?//\",\n",
    "        \"]\",\n",
    "        \"}\",\n",
    "        \"|c\",\n",
    "        \">+\",\n",
    "        \"||\",\n",
    "        \"}//\",\n",
    "        \"&//\",\n",
    "        \"//.\",\n",
    "        \")\",\n",
    "        \"|r\",\n",
    "        \">=\",\n",
    "        \"//+\",\n",
    "        \"<+\",\n",
    "        \"?//]\",\n",
    "        \"//]\",\n",
    "        \"//=\",\n",
    "        \"!//\",\n",
    "        \"?//=\",\n",
    "        \"!//=\",\n",
    "        \"//)\",\n",
    "        \"|a\",\n",
    "        \"&?//\",\n",
    "        \"!//]\",\n",
    "        \"&//]\",\n",
    "        \"//&\",\n",
    "        \"?//]\",\n",
    "        \"!//)\",\n",
    "        \"&?//]\",\n",
    "        \"{\",\n",
    "        \"(\",\n",
    "        \"[\",\n",
    "        \"&\",\n",
    "        \"||e\",\n",
    "        \"<{\",\n",
    "        \"//t\",\n",
    "        \"{|c\",\n",
    "        \"|}\",\n",
    "        \"|\",\n",
    "        \"/\",\n",
    "        \"?\",\n",
    "        \"+\",\n",
    "        \",\",\n",
    "    ]\n",
    "\n",
    "    with open(conllu_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read().strip().split('\\n\\n')\n",
    "\n",
    "    corrected_data = []\n",
    "\n",
    "    for sentence in data:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        metadata = []\n",
    "        token_lines = []\n",
    "        for line in lines:\n",
    "            if line.startswith('#'):\n",
    "                metadata.append(line)\n",
    "            else:\n",
    "                token_lines.append(line.split('\\t'))\n",
    "\n",
    "        sent_id = None\n",
    "        for m in metadata:\n",
    "            if m.startswith(\"# sent_id =\"):\n",
    "                sent_id = m.split(\" = \")[1]\n",
    "                break\n",
    "\n",
    "        if sent_id and \"__\" in sent_id:\n",
    "            sent_id_key = sent_id.split(\"__\")[1]\n",
    "            file_name = sent_id.split(\"__\")[0]\n",
    "            if file_name.startswith(\"ABJ\"):\n",
    "                file_name = \"_\".join(file_name.split(\"_\")[:3])\n",
    "            else:\n",
    "                file_name = \"_\".join(file_name.split(\"_\")[:2])\n",
    "            \n",
    "            file_name = file_name + \"-merged\"\n",
    "\n",
    "            previous_end = None\n",
    "\n",
    "            if file_name in sentences and sent_id_key in sentences[file_name]:\n",
    "                for token in token_lines:\n",
    "                    token_id = sent_id_key + \":\" + token[0]\n",
    "                    token_id_plus_1 = sent_id_key + \":\" + str(int(token[0]) + 1)\n",
    "\n",
    "                    # print(\"1. \", token_id, token)\n",
    "                    if token_id in sentences[file_name][sent_id_key][\"words\"]:\n",
    "                        word_info = sentences[file_name][sent_id_key][\"words\"][token_id]\n",
    "                        word_info_plus_1 = sentences[file_name][sent_id_key][\"words\"].get(token_id_plus_1, None)\n",
    "                        \n",
    "                        # print(\" 2. \", word_info)\n",
    "                        if token[1] == word_info[2] and word_info[2]:\n",
    "                            start, end, token_text = word_info\n",
    "                            token[1] = token_text\n",
    "                            previous_end = end\n",
    "                            if token[-1] == \"_\":\n",
    "                                token[-1] = f\"AlignBegin={int(start * 1000)}|AlignEnd={int(end * 1000)}\"  # Convertir les secondes en millisecondes\n",
    "                            else:\n",
    "                                token[-1] = f\"AlignBegin={int(start * 1000)}|AlignEnd={int(end * 1000)}\"\n",
    "\n",
    "                        elif word_info_plus_1 and token[1] == word_info_plus_1[2] and word_info_plus_1[2]:\n",
    "                            start, end, token_text = word_info_plus_1\n",
    "                            token[1] = token_text\n",
    "                            previous_end = end\n",
    "                            if token[-1] == \"_\":\n",
    "                                token[-1] = f\"AlignBegin={int(start * 1000)}|AlignEnd={int(end * 1000)}\"\n",
    "                            else:\n",
    "                                token[-1] = f\"AlignBegin={int(start * 1000)}|AlignEnd={int(end * 1000)}\"\n",
    "\n",
    "                        elif word_info_plus_1 and token[1] != word_info[2] and token[1] != word_info_plus_1[2]:\n",
    "                            start, end, token_text = word_info_plus_1\n",
    "                            if previous_end and previous_end < start:\n",
    "                                token[-1] = f\"AlignBegin={int(previous_end * 1000)}|AlignEnd={int(start * 1000)}\"\n",
    "                                previous_end = start\n",
    "                            else:\n",
    "                                token[-1] = f\"AlignBegin=X|AlignEnd=X\"\n",
    "\n",
    "                        else:\n",
    "                            token[-1] = f\"AlignBegin=X|AlignEnd=X\"\n",
    "\n",
    "                    elif token[1] in punctuation_list:\n",
    "                        if previous_end is not None:\n",
    "                            token[-1] = f\"AlignBegin={int(previous_end * 1000)}|AlignEnd={int(previous_end * 1000)}\"\n",
    "                        else:\n",
    "                            token[-1] = f\"AlignBegin=X|AlignEnd=X\"\n",
    "\n",
    "                    else:\n",
    "                        token[-1] = f\"AlignBegin=X|AlignEnd=X\"\n",
    "\n",
    "                corrected_lines = ['\\t'.join(token) for token in token_lines]\n",
    "                corrected_sentence = '\\n'.join(metadata + corrected_lines)\n",
    "                corrected_data.append(corrected_sentence)\n",
    "            else:\n",
    "                corrected_data.append(sentence)\n",
    "        else:\n",
    "            corrected_data.append(sentence)\n",
    "\n",
    "    corrected_conllu = '\\n\\n'.join(corrected_data)\n",
    "\n",
    "    with open(conllu_path.replace('.conllu', '.conllu'), 'w', encoding='utf-8') as f:\n",
    "        f.write(corrected_conllu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_alignments_delete_x(conllu_path:str):\n",
    "    \"\"\"\n",
    "    Adjusts the alignments in a CONLL-U file by removing 'X' values and replacing them with valid values.\n",
    "\n",
    "    Parameters:\n",
    "    conllu_path (str): The path of the CONLL-U file to adjust.\n",
    "    \"\"\"\n",
    "    with open(conllu_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read().strip().split('\\n\\n')\n",
    "\n",
    "    adjusted_data = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        lines = sentence.split('\\n')\n",
    "        first_alignbegin = None\n",
    "        last_alignend = None\n",
    "\n",
    "        # Get the alignments of each token\n",
    "        alignments = []\n",
    "        for line in lines:\n",
    "            if '\\t' in line:\n",
    "                parts = line.split('\\t')\n",
    "                align_info = parts[-1]\n",
    "                alignbegin_match = re.search(r'AlignBegin=(\\d+|X)', align_info)\n",
    "                alignend_match = re.search(r'AlignEnd=(\\d+|X)', align_info)\n",
    "                if alignbegin_match and alignend_match:\n",
    "                    alignbegin = alignbegin_match.group(1)\n",
    "                    alignend = alignend_match.group(1)\n",
    "                    if alignbegin != 'X':\n",
    "                        alignbegin = int(alignbegin)\n",
    "                    else:\n",
    "                        alignbegin = None\n",
    "                    if alignend != 'X':\n",
    "                        alignend = int(alignend)\n",
    "                    else:\n",
    "                        alignend = None\n",
    "                    alignments.append((alignbegin, alignend))\n",
    "                    if first_alignbegin is None and alignbegin is not None:\n",
    "                        first_alignbegin = alignbegin\n",
    "                    if alignend is not None:\n",
    "                        last_alignend = alignend\n",
    "\n",
    "        # Make sure the first and last alignments are defined\n",
    "        if first_alignbegin is None:\n",
    "            first_alignbegin = 0  # Default to 0 if no value found\n",
    "        if last_alignend is None and alignments:\n",
    "            last_alignend = alignments[-1][1] if alignments[-1][1] is not None else 0\n",
    "\n",
    "        if not alignments:\n",
    "            adjusted_data.append(sentence)\n",
    "            continue\n",
    "\n",
    "        # Adjust missing alignments\n",
    "        for i, (alignbegin, alignend) in enumerate(alignments):\n",
    "            if alignbegin is None:\n",
    "                if i > 0 and alignments[i-1][1] is not None:\n",
    "                    alignbegin = alignments[i-1][1]\n",
    "                else:\n",
    "                    alignbegin = first_alignbegin\n",
    "            if alignend is None:\n",
    "                if i < len(alignments) - 1 and alignments[i+1][0] is not None:\n",
    "                    alignend = alignments[i+1][0]\n",
    "                else:\n",
    "                    alignend = last_alignend\n",
    "            alignments[i] = (alignbegin, alignend)\n",
    "\n",
    "        # Set alignbegin = alignend if alignbegin > alignend\n",
    "        for i, (alignbegin, alignend) in enumerate(alignments):\n",
    "            if alignbegin > alignend:\n",
    "                alignend = alignbegin\n",
    "            alignments[i] = (alignbegin, alignend)\n",
    "\n",
    "        for i in range(1, len(alignments)):\n",
    "            alignbegin, alignend = alignments[i]\n",
    "            # print(\"i-1 :\" , i-1, alignments[i-1])\n",
    "            # print(\"old :\", i, alignbegin, alignend, alignments[i])\n",
    "            prev_alignend = alignments[i-1][1]\n",
    "            if i + 1 < len(alignments) and prev_alignend > alignments[i+1][1]:\n",
    "                prev_alignend = alignments[i-1][0]\n",
    "            # print(\"prev : \", prev_alignend)\n",
    "            if alignbegin is not None and alignend is not None:\n",
    "                if prev_alignend is not None:\n",
    "                    if alignbegin != prev_alignend or alignbegin > alignend:\n",
    "                        if alignbegin == alignend:\n",
    "                            alignbegin = prev_alignend\n",
    "                            alignments[i] = (alignbegin, alignbegin)\n",
    "                        else:\n",
    "                            alignbegin = prev_alignend\n",
    "                            alignments[i] = (alignbegin, alignend)\n",
    "                        # print(\"new : \", alignbegin, alignend, alignments[i])\n",
    "\n",
    "        # Adjust the lines with alignments\n",
    "        adjusted_lines = []\n",
    "        alignment_index = 0\n",
    "        for line in lines:\n",
    "            if '\\t' in line:\n",
    "                parts = line.split('\\t')\n",
    "                align_info = parts[-1]\n",
    "                if 'AlignBegin' in align_info and 'AlignEnd' in align_info:\n",
    "                    alignbegin, alignend = alignments[alignment_index]\n",
    "                    # print(alignbegin, alignend, alignments[alignment_index])\n",
    "                    alignment_index += 1\n",
    "                    if parts[3] == 'PUNCT' and parts[1] != \"#\":\n",
    "                        alignend = alignbegin\n",
    "                    else:\n",
    "                        if alignbegin is not None and alignbegin != first_alignbegin:\n",
    "                            alignbegin = alignbegin\n",
    "                        if alignend is not None and alignend != last_alignend:\n",
    "                            alignend = alignend\n",
    "                    if alignbegin is not None:\n",
    "                        alignbegin = max(0, alignbegin)\n",
    "                    if alignend is not None:\n",
    "                        alignend = max(0, alignend)\n",
    "                    parts[-1] = f'AlignBegin={alignbegin}|AlignEnd={alignend}'\n",
    "                adjusted_lines.append('\\t'.join(parts))\n",
    "            else:\n",
    "                adjusted_lines.append(line)\n",
    "        # print(adjusted_lines)\n",
    "        adjusted_data.append('\\n'.join(adjusted_lines))\n",
    "\n",
    "    with open(conllu_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n\\n'.join(adjusted_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "input_textgrid_directory = \"../data/TG_WAV/\"\n",
    "input_conllu_directory = \"../data/SUD_Naija-NSC-master/gold_nongold/\"\n",
    "conllu_output_directory = \"../data/conllu_output/gold_nongold/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correction des transcriptions en ajoutant les # manquants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ABJ_GWA_14_Mary-Lifestory_MG.conllu with ../data/TG_WAV/ABJ_GWA_14/new/ABJ_GWA_14-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/KAD_17_Turkeys_MG.conllu with ../data/TG_WAV/KAD_17/new/KAD_17-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZL_08_Edewor-Lifestory_MG.conllu with ../data/TG_WAV/WAZL_08/new/WAZL_08-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/JOS_19_Bukuru_MG.conllu with ../data/TG_WAV/JOS_19/new/JOS_19-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/PRT_07_Drummer_MG.conllu with ../data/TG_WAV/PRT_07/new/PRT_07-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZL_03_News-On-Gmns_MG.conllu with ../data/TG_WAV/WAZL_03/new/WAZL_03-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/BEN_36_Clever-Girl_MG.conllu with ../data/TG_WAV/BEN_36/new/BEN_36-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ENU_34_Malaysia-Guy_MG.conllu with ../data/TG_WAV/ENU_34/new/ENU_34-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ABJ_GWA_08_David-Lifestory_MG.conllu with ../data/TG_WAV/ABJ_GWA_08/new/ABJ_GWA_08-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ENU_02_Christmas-At-New-Berries_MG.conllu with ../data/TG_WAV/ENU_02/new/ENU_02-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_31_Lens-Sermon_MG.conllu with ../data/TG_WAV/IBA_31/new/IBA_31-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZK_08_Fuel-Price-Increase_MG.conllu with ../data/TG_WAV/WAZK_08/new/WAZK_08-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZL_15_MC-Abi_MG.conllu with ../data/TG_WAV/WAZL_15/new/WAZL_15-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZA_09_Tv-News_MG.conllu with ../data/TG_WAV/WAZA_09/new/WAZA_09-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZP_04_Ponzi-Scheme_MG.conllu with ../data/TG_WAV/WAZP_04/new/WAZP_04-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZL_02_Good-Morning-Nigeria_MG.conllu with ../data/TG_WAV/WAZL_02/new/WAZL_02-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZA_08_Body-Matter_MG.conllu with ../data/TG_WAV/WAZA_08/new/WAZA_08-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/JOS_14_Chibozor-View-About-Nigeria_MG.conllu with ../data/TG_WAV/JOS_14/new/JOS_14-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_21_Obodo-Barracks_MG.conllu with ../data/TG_WAV/IBA_21/new/IBA_21-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZA_05_Big-Mo_MG.conllu with ../data/TG_WAV/WAZA_05/new/WAZA_05-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_04_Alaska-Pepe_MG.conllu with ../data/TG_WAV/IBA_04/new/IBA_04-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZP_07_Imonirhua-Lifestory_MG.conllu with ../data/TG_WAV/WAZP_07/new/WAZP_07-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_20_Bose-Alade_MG.conllu with ../data/TG_WAV/IBA_20/new/IBA_20-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ENU_37_Dmoris-Restaurant_MG.conllu with ../data/TG_WAV/ENU_37/new/ENU_37-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ENU_33_A-Beg_MG.conllu with ../data/TG_WAV/ENU_33/new/ENU_33-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ABJ_GWA_03_Cost-Of-Living-In-Abuja_MG.conllu with ../data/TG_WAV/ABJ_GWA_03/new/ABJ_GWA_03-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/LAG_07_Johns-Biography_MG.conllu with ../data/TG_WAV/LAG_07/new/LAG_07-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_32_Tori-By-Samuel_MG.conllu with ../data/TG_WAV/IBA_32/new/IBA_32-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ENU_22_Barman-Interview_MG.conllu with ../data/TG_WAV/ENU_22/new/ENU_22-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZP_03_Education_MG.conllu with ../data/TG_WAV/WAZP_03/new/WAZP_03-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ABJ_INF_10_Women-Battering_MG.conllu with ../data/TG_WAV/ABJ_INF_10/new/ABJ_INF_10-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ENU_09_Angry-Neighbours_MG.conllu with ../data/TG_WAV/ENU_09/new/ENU_09-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/LAG_21_I-Like-Stout_MG.conllu with ../data/TG_WAV/LAG_21/new/LAG_21-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_40_Christ-Passion-Prologue_MG.conllu with ../data/TG_WAV/IBA_40/new/IBA_40-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZK_07_As-E-Dey-Hot-News-Read_MG.conllu with ../data/TG_WAV/WAZK_07/new/WAZK_07-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZA_03_Obi-Lifestory_MG.conllu with ../data/TG_WAV/WAZA_03/new/WAZA_03-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_03_Womanisers_MG.conllu with ../data/TG_WAV/IBA_03/new/IBA_03-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/KAD_09_Kabir-Gymnasium_MG.conllu with ../data/TG_WAV/KAD_09/new/KAD_09-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/PRT_11_A-Man-Named-Jesus_MG.conllu with ../data/TG_WAV/PRT_11/new/PRT_11-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/BEN_14_BronzeFM-News_MG.conllu with ../data/TG_WAV/BEN_14/new/BEN_14-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/JOS_20_Beauty-Of-Jos_MG.conllu with ../data/TG_WAV/JOS_20/new/JOS_20-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ABJ_NOU_02_Gimba-Lifestory_MG.conllu with ../data/TG_WAV/ABJ_NOU_02/new/ABJ_NOU_02-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ENU_01_Salomis-Egusi-Soup-Recipe_MG.conllu with ../data/TG_WAV/ENU_01/new/ENU_01-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/LAG_31_Road-Safety_MG.conllu with ../data/TG_WAV/LAG_31/new/LAG_31-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/KAD_03_Why-Men-Watch-Football_MG.conllu with ../data/TG_WAV/KAD_03/new/KAD_03-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ABJ_GWA_12_Accident_MG.conllu with ../data/TG_WAV/ABJ_GWA_12/new/ABJ_GWA_12-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ONI_07_Dis-Year-Na-My-Year_MG.conllu with ../data/TG_WAV/ONI_07/new/ONI_07-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/LAG_37_Soap-Making_MG.conllu with ../data/TG_WAV/LAG_37/new/LAG_37-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZA_01_Triplea-Sports_MG.conllu with ../data/TG_WAV/WAZA_01/new/WAZA_01-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_23_Bitter-Leaf-Soup_MG.conllu with ../data/TG_WAV/IBA_23/new/IBA_23-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/LAG_27_Shawarma_MG.conllu with ../data/TG_WAV/LAG_27/new/LAG_27-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/BEN_08_Egusi-And-Banga-Soup_MG.conllu with ../data/TG_WAV/BEN_08/new/BEN_08-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/KAD_12_Mechanic-At-Work_MG.conllu with ../data/TG_WAV/KAD_12/new/KAD_12-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/LAG_12_Insurance_MG.conllu with ../data/TG_WAV/LAG_12/new/LAG_12-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/LAG_11_Adeniyi-Lifestory_MG.conllu with ../data/TG_WAV/LAG_11/new/LAG_11-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/KAD_13_Entering-University_MG.conllu with ../data/TG_WAV/KAD_13/new/KAD_13-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/BEN_09_Tailoring-Immunization_MG.conllu with ../data/TG_WAV/BEN_09/new/BEN_09-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_02_Igwe-Festival_MG.conllu with ../data/TG_WAV/IBA_02/new/IBA_02-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ONI_10_Sport-Commentary_MG.conllu with ../data/TG_WAV/ONI_10/new/ONI_10-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_15_Electrician_MG.conllu with ../data/TG_WAV/IBA_15/new/IBA_15-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/JOS_12_How-To-Prepare-Gote-Soup_MG.conllu with ../data/TG_WAV/JOS_12/new/JOS_12-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/JOS_01_People-Of-Plateau_MG.conllu with ../data/TG_WAV/JOS_01/new/JOS_01-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ABJ_INF_12_Evictions_MG.conllu with ../data/TG_WAV/ABJ_INF_12/new/ABJ_INF_12-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_34_News-Report-By-Samuel_MG.conllu with ../data/TG_WAV/IBA_34/new/IBA_34-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/KAD_15_Money-Wahala_MG.conllu with ../data/TG_WAV/KAD_15/new/KAD_15-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/KAD_10_Egusi-Soup_MG.conllu with ../data/TG_WAV/KAD_10/new/KAD_10-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_33_News-Comments_MG.conllu with ../data/TG_WAV/IBA_33/new/IBA_33-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_41_Christ-Passion-Finale_MG.conllu with ../data/TG_WAV/IBA_41/new/IBA_41-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ONI_27_A-Hotelier-Interview_MG.conllu with ../data/TG_WAV/ONI_27/new/ONI_27-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/WAZA_10_Bluetooth-Lifestory_MG.conllu with ../data/TG_WAV/WAZA_10/new/WAZA_10-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/PRT_05_Ghetto-Life_MG.conllu with ../data/TG_WAV/PRT_05/new/PRT_05-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/PRT_02_Food-And-Health_MG.conllu with ../data/TG_WAV/PRT_02/new/PRT_02-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ABJ_GWA_09_Journalism_MG.conllu with ../data/TG_WAV/ABJ_GWA_09/new/ABJ_GWA_09-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ABJ_GWA_10_Steven-Lifestory_MG.conllu with ../data/TG_WAV/ABJ_GWA_10/new/ABJ_GWA_10-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/BEN_34_Tale_MG.conllu with ../data/TG_WAV/BEN_34/new/BEN_34-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ABJ_GWA_06_Ugo-Lifestory_MG.conllu with ../data/TG_WAV/ABJ_GWA_06/new/ABJ_GWA_06-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/BEN_02_Andrew-Lifestory_MG.conllu with ../data/TG_WAV/BEN_02/new/BEN_02-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/IBA_01_Fola-Lifestory_MG.conllu with ../data/TG_WAV/IBA_01/new/IBA_01-merged.TextGrid\n",
      "\n",
      " Processing ../data/SUD_Naija-NSC-master/gold_nongold/ONI_26_News-Highlights_MG.conllu with ../data/TG_WAV/ONI_26/new/ONI_26-merged.TextGrid\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(conllu_output_directory):\n",
    "    os.makedirs(conllu_output_directory)\n",
    "\n",
    "for root, dirs, files in os.walk(input_conllu_directory):\n",
    "    # print(root)\n",
    "    for file in files:\n",
    "        # print(file)\n",
    "        if file.endswith(\"_MG.conllu\") or file.endswith(\"_M.conllu\"):\n",
    "            conllu_file = os.path.join(root, file)\n",
    "\n",
    "            # print(conllu_file)\n",
    "            if file.startswith(\"ABJ\"):\n",
    "                textgrid_name = \"_\".join(file.split(\"_\")[:3])\n",
    "            else:\n",
    "                textgrid_name = \"_\".join(file.split(\"_\")[:2])\n",
    "\n",
    "            textgrid_path = os.path.join(input_textgrid_directory, textgrid_name + \"/new/\" + textgrid_name + \"-merged.TextGrid\")\n",
    "\n",
    "            # print(textgrid_path)\n",
    "\n",
    "            if os.path.exists(textgrid_path):\n",
    "                # if \"JOS_38\" in textgrid_path:\n",
    "                print(f\"\\n Processing {conllu_file} with {textgrid_path}\")\n",
    "                sentences_dict = load_textgrid(textgrid_path)\n",
    "                conllu_output_file = conllu_output_directory + file\n",
    "                old_text, new_text = update_sent_text_conllu(conllu_file, sentences_dict, conllu_output_file)\n",
    "                update_conllu_id(conllu_output_file, conllu_output_file, old_text)\n",
    "                sentences_all = load_textgrid(textgrid_path, alignment=True)\n",
    "                correct_alignements(sentences_all, conllu_output_file)\n",
    "                # adjust_alignments_delete_x(conllu_output_file)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
